{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MAML enhanced model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision.models.resnet import resnet50\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import draw_trajectory, PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-03T17:27:36.743208Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJJbXRdVY41t"
   },
   "source": [
    "Prepare Data path and load cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6pAQR-VY41t",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-03T17:27:36.799622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_version': 4, 'model_params': {'model_architecture': 'resnet50', 'history_num_frames': 0, 'future_num_frames': 20, 'step_time': 0.1, 'render_ego_history': True}, 'raster_params': {'raster_size': [224, 224], 'pixel_size': [0.5, 0.5], 'ego_center': [0.25, 0.5], 'map_type': 'py_semantic', 'satellite_map_key': 'aerial_map/aerial_map.png', 'semantic_map_key': 'semantic_map/semantic_map.pb', 'dataset_meta_key': 'meta.json', 'filter_agents_threshold': 0.5, 'disable_traffic_light_faces': False, 'set_origin_to_bottom': True}, 'train_data_loader': {'key': 'scenes/train.zarr', 'batch_size': 12, 'shuffle': True, 'num_workers': 0}, 'val_data_loader': {'key': 'scenes/validate.zarr', 'batch_size': 12, 'shuffle': False, 'num_workers': 0}, 'test_data_loader': {'key': 'scenes/test.zarr', 'batch_size': 12, 'shuffle': False, 'num_workers': 0}, 'sample_data_loader': {'key': 'scenes/sample.zarr', 'batch_size': 12, 'shuffle': False, 'num_workers': 0}, 'train_params': {'checkpoint_every_n_steps': 10000, 'max_num_steps': 20, 'epochs': 20, 'eval_every_n_steps': 10000}}\n"
     ]
    }
   ],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"lyft-motion-prediction-autonomous-vehicles\"\n",
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./agent_motion_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peU_Im9oY41u"
   },
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff9fksj7Y41u",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-03T17:27:36.868340Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(cfg: Dict) -> torch.nn.Module:\n",
    "    model = resnet50()\n",
    "\n",
    "    # change input channels number to match the rasterizer's output\n",
    "    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "    num_in_channels = 3 + num_history_channels\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        num_in_channels,\n",
    "        model.conv1.out_channels,\n",
    "        kernel_size=model.conv1.kernel_size,\n",
    "        stride=model.conv1.stride,\n",
    "        padding=model.conv1.padding,\n",
    "        bias=False,\n",
    "    )\n",
    "    # change output size to (X, Y) * number of future states\n",
    "    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "    model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w51ydrIoY41u",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def forward(data, model, device, criterion):\n",
    "    # inputs = data[\"image\"].to(device)\n",
    "    inputs = data[\"image\"].to(device)\n",
    "\n",
    "    # Min-max scaling\n",
    "    inputs_min = torch.min(inputs)\n",
    "    inputs_max = torch.max(inputs)\n",
    "    inputs = (inputs - inputs_min) / (inputs_max - inputs_min)\n",
    "\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n",
    "    targets = data[\"target_positions\"].to(device)\n",
    "    # Forward pass\n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    loss = criterion(outputs, targets)\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qea6szT7Y41u"
   },
   "source": [
    "### Load the Train Data\n",
    "- loading the `zarr` into a `ChunkedDataset` object. This object has a reference to the different arrays into the zarr (e.g. agents and traffic lights);\n",
    "- wrapping the `ChunkedDataset` into an `AgentDataset`, which inherits from torch `Dataset` class;\n",
    "- passing the `AgentDataset` into a torch `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjiq5IlHY41v",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# ===== INIT DATASET\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\n",
    "\n",
    "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"], \n",
    "                             num_workers=train_cfg[\"num_workers\"])\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmNi74bRY41v",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# ==== INIT MODEL\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = build_model(cfg).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "criterion = nn.MSELoss(reduction=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqj4WYWwY41v"
   },
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import learn2learn as l2l\n",
    "\n",
    "model = build_model(cfg)  # Replace YourModel with your own model\n",
    "maml = l2l.algorithms.MAML(model, lr=0.0008, first_order=False)  # Create MAML instance\n",
    "optimizer = torch.optim.SGD(maml.parameters(), lr=0.001)  # Replace with your optimizer\n",
    "\n",
    "tr_it = iter(train_dataloader)\n",
    "progress_bar = tqdm(range(20))\n",
    "losses_train_m_avg = []\n",
    "losses_train_m = []\n",
    "\n",
    "adaptation_steps = 2\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Learning rate scheduler\n",
    "\n",
    "training_start_time = time.time()\n",
    "for _ in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    # Perform fast adaptation using MAML\n",
    "    learner = maml.clone()  # Create a clone of the model for adaptation\n",
    "\n",
    "    inputs = data[\"image\"].to(device)\n",
    "\n",
    "    # Min-max scaling\n",
    "    inputs_min = torch.min(inputs)\n",
    "    inputs_max = torch.max(inputs)\n",
    "    adaptation_data = (inputs - inputs_min) / (inputs_max - inputs_min)\n",
    "    # adaptation_data = data[\"image\"].to(device)  # Modify the data as needed for adaptation\n",
    "    adaptation_labels = data[\"target_positions\"].to(device)  # Modify the labels as needed for adaptation\n",
    "\n",
    "    for step in range(adaptation_steps):\n",
    "        loss = criterion(learner(adaptation_data).reshape(adaptation_labels.shape), adaptation_labels)\n",
    "        learner.adapt(loss.mean())\n",
    "\n",
    "    # Compute loss and update model\n",
    "    loss, _ = forward(data, learner, device, criterion)  # Use the adapted learner for forward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses_train_m_avg.append(np.mean(losses_train_m))\n",
    "    losses_train_m.append(loss.item())\n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train_m)}\")\n",
    "\n",
    "    scheduler.step()  # Update the learning rate\n",
    "\n",
    "print('Training finished, took {:.2f}s'.format(time.time() - training_start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Plot Loss Curve\n",
    "We can plot the train loss against the iterations (batch-wise)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(losses_train_m)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses_train_m)), losses_train_m, label=\"train loss\")\n",
    "plt.ylim(0, 45)  # Set the y-axis limits\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses_train_m_avg)), losses_train_m_avg, label=\"train loss\")\n",
    "plt.ylim(0, 45)  # Set the y-axis limits\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # ===== GENERATE AND LOAD CHOPPED DATASET\n",
    "num_frames_to_chop = 50\n",
    "eval_cfg = cfg[\"sample_data_loader\"]\n",
    "eval_base_path = create_chopped_dataset(dm.require(eval_cfg[\"key\"]), cfg[\"raster_params\"][\"filter_agents_threshold\"],\n",
    "                              num_frames_to_chop, cfg[\"model_params\"][\"future_num_frames\"], MIN_FUTURE_STEPS)\n",
    "# eval_base_path = \"lyft-motion-prediction-autonomous-vehicles/scenes/sample_chopped_50\""
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "eval_zarr_path = str(Path(eval_base_path) / \"sample.zarr\")\n",
    "eval_mask_path = str(Path(eval_base_path) / \"mask.npz\")\n",
    "eval_gt_path = str(Path(eval_base_path) / \"gt.csv\")\n",
    "\n",
    "eval_zarr = ChunkedDataset(eval_zarr_path).open()\n",
    "eval_mask = np.load(eval_mask_path)[\"arr_0\"]\n",
    "# ===== INIT DATASET AND LOAD MASK\n",
    "eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer, agents_mask=eval_mask)\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=eval_cfg[\"shuffle\"], batch_size=eval_cfg[\"batch_size\"],\n",
    "                             num_workers=eval_cfg[\"num_workers\"])\n",
    "print(eval_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Storing Predictions\n",
    "There is a small catch to be aware of when saving the model predictions. The output of the models are coordinates in `agent` space and we need to convert them into displacements in `world` space.\n",
    "\n",
    "To do so, we first convert them back into the `world` space and we then subtract the centroid coordinates."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ==== EVAL LOOP\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# store information for evaluation\n",
    "future_coords_offsets_pd = []\n",
    "timestamps = []\n",
    "agent_ids = []\n",
    "\n",
    "progress_bar = tqdm(eval_dataloader)\n",
    "for data in progress_bar:\n",
    "    _, ouputs = forward(data, model, device, criterion)\n",
    "\n",
    "    # convert agent coordinates into world offsets\n",
    "    agents_coords = ouputs.cpu().numpy()\n",
    "    world_from_agents = data[\"world_from_agent\"].numpy()\n",
    "    centroids = data[\"centroid\"].numpy()\n",
    "    coords_offset = transform_points(agents_coords, world_from_agents) - centroids[:, None, :2]\n",
    "\n",
    "    future_coords_offsets_pd.append(np.stack(coords_offset))\n",
    "    timestamps.append(data[\"timestamp\"].numpy().copy())\n",
    "    agent_ids.append(data[\"track_id\"].numpy().copy())\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_path = \"lyft-motion-prediction-autonomous-vehicles/pred.csv\"\n",
    "\n",
    "write_pred_csv(pred_path,\n",
    "               timestamps=np.concatenate(timestamps),\n",
    "               track_ids=np.concatenate(agent_ids),\n",
    "               coords=np.concatenate(future_coords_offsets_pd),\n",
    "              )"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Perform Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_gt_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01ml5kit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m average_displacement_error_mean\n\u001B[0;32m----> 3\u001B[0m metrics \u001B[38;5;241m=\u001B[39m compute_metrics_csv(\u001B[43meval_gt_path\u001B[49m, pred_path, [neg_multi_log_likelihood, time_displace, average_displacement_error_mean])\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m metric_name, metric_mean \u001B[38;5;129;01min\u001B[39;00m metrics\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(metric_name, metric_mean)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'eval_gt_path' is not defined"
     ]
    }
   ],
   "source": [
    "from l5kit.evaluation.metrics import average_displacement_error_mean\n",
    "\n",
    "metrics = compute_metrics_csv(eval_gt_path, pred_path, [neg_multi_log_likelihood, time_displace, average_displacement_error_mean])\n",
    "for metric_name, metric_mean in metrics.items():\n",
    "    print(metric_name, metric_mean)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-03T17:27:36.869676Z",
     "start_time": "2023-06-03T17:27:36.810603Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualise Results\n",
    "We can also visualise some results from the ego (AV) point of view for those frames of interest (the 100th of each scene).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# build a dict to retrieve future trajectories from GT\n",
    "gt_rows = {}\n",
    "for row in read_gt_csv(eval_gt_path):\n",
    "    gt_rows[row[\"track_id\"] + row[\"timestamp\"]] = row[\"coord\"]\n",
    "\n",
    "eval_ego_dataset = EgoDataset(cfg, eval_dataset.dataset, rasterizer)\n",
    "\n",
    "\n",
    "for frame_number in range(99, len(eval_zarr.frames), 100):  # start from last frame of scene_0 and increase by 100\n",
    "    agent_indices = eval_dataset.get_frame_indices(frame_number)\n",
    "    if not len(agent_indices):\n",
    "        continue\n",
    "\n",
    "    # get AV point-of-view frame\n",
    "    data_ego = eval_ego_dataset[frame_number]\n",
    "    im_ego = rasterizer.to_rgb(data_ego[\"image\"].transpose(1, 2, 0))\n",
    "    center = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "\n",
    "    predicted_positions = []\n",
    "    target_positions = []\n",
    "\n",
    "    for v_index in agent_indices:\n",
    "        data_agent = eval_dataset[v_index]\n",
    "\n",
    "        out_net = model(torch.from_numpy(data_agent[\"image\"]).unsqueeze(0).to(device))\n",
    "        out_pos = out_net[0].reshape(-1, 2).detach().cpu().numpy()\n",
    "        # store absolute world coordinates\n",
    "        predicted_positions.append(transform_points(out_pos, data_agent[\"world_from_agent\"]))\n",
    "        # retrieve target positions from the GT and store as absolute coordinates\n",
    "        track_id, timestamp = data_agent[\"track_id\"], data_agent[\"timestamp\"]\n",
    "        target_positions.append(gt_rows[str(track_id) + str(timestamp)] + data_agent[\"centroid\"][:2])\n",
    "\n",
    "\n",
    "    # convert coordinates to AV point-of-view so we can draw them\n",
    "    predicted_positions = transform_points(np.concatenate(predicted_positions), data_ego[\"raster_from_world\"])\n",
    "    target_positions = transform_points(np.concatenate(target_positions), data_ego[\"raster_from_world\"])\n",
    "\n",
    "    draw_trajectory(im_ego, predicted_positions, PREDICTED_POINTS_COLOR)\n",
    "    draw_trajectory(im_ego, target_positions, TARGET_POINTS_COLOR)\n",
    "\n",
    "    plt.imshow(im_ego)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
